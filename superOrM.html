<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>Balanced OrM</title>
  <style type="text/css">
      code{white-space: pre-wrap;}
      span.smallcaps{font-variant: small-caps;}
      div.line-block{white-space: pre-line;}
      div.column{display: inline-block; vertical-align: top; width: 50%;}
  </style>
  <link rel="stylesheet" href="./md_html.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-AMS_CHTML-full" type="text/javascript"></script>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<header>
<h1 class="title">Balanced OrM</h1>
</header>
<h2 id="model-modifications">Model modifications</h2>
<p>We find the OrMachine posterior based on the Hamming Distance between the data <span class="math inline">\(X\)</span> and the prediction <span class="math inline">\(\max(Z \cdot U, 1)\)</span>. For an observation <span class="math inline">\(x_{nd}\)</span>, we have <span class="math display">\[\begin{align}
    d_{\text{OrM}} = 1-\tilde{x}_{nd}\tilde{g}_{nd}
\end{align}\]</span> where <span class="math inline">\(g_{nd} = 1 - \prod\limits_l (1-z_{nl}u_{ld})\)</span>. Then we can define a distribution over the latent factors. <span class="math display">\[\begin{align}
    p(z_{nl}|.) \propto \exp\left[-\frac{1}{2} \lambda \; 
        \sum\limits_d d_{\text{OrM}}^{(nd)} \right]
\end{align}\]</span> Normalising recovers the good old OrMachine posterior <span class="math display">\[\begin{align}
    p(z_{nl}|.) = \sigma\left[\frac{1}{2}\lambda \sum\limits_d
        (d_{\text{OrM}}^{z_{nl}=1} - d_{\text{OrM}}^{z_{nl}=0}) \right]
    = \sigma\left[\lambda \sum\limits_d \tilde{x}_{nd} u_{ld}
        \prod\limits_{l&#39;\neq l} (1-z_{nl}u_{ld}) \right]
\end{align}\]</span></p>
<p>Now letâ€™s assume we have many type II errors in the data, but only few type I errors. In that case we believe positive observations more then negative observations and thus want to control the associated noise separately. We can define the distance <span class="math display">\[\begin{align}
    d_{\text{BalancedOrM}} = \sum\limits_d
     (1-\tilde{x}_{nd}\tilde{g}_{nd}) \lambda
     \left[ x_{nd} + \frac{1}{s} (x_{nd}-1)
     \right]
\end{align}\]</span> Now the contribution of positive observations is diminished by <span class="math inline">\(1/s\)</span>. This leads to the conditionals <span class="math display">\[\begin{equation}
    p(z_{nl}=1|.) = \sigma\left[ \sum\limits_d
        \lambda 
        \left[x_{nd} + \frac{1}{s} (x_{nd}-1) \right]
        \tilde{x}_{nd} u_{ld}
        \prod\limits_{l&#39;\neq l} (1-z_{nl}u_{ld}) \right]
\end{equation}\]</span></p>
<p>The corresponding likelihood a bit strange, conditioned on the true value of the data <span class="math inline">\(\bar{x}\)</span>.</p>
<span class="math display">\[\begin{align}
p(x_{nd}|.) = \sigma\left[\tilde{x}_{nd} \tilde{g}_{nd} \lambda
    \left(\bar{x}_{nd} + \frac{\bar{x}_{nd}-1}{s} \right) \right]
\end{align}\]</span>
<p>The conditional MAP estimate for <span class="math inline">\(\lambda\)</span> becomes <span class="math display">\[\begin{align}
    \sigma(\lambda) = \frac{TP + \frac{TN}{s}}{TP + FN + \frac{TN + FP}{s}}
\end{align}\]</span></p>
<p>For now we consider s a fixed hyperparameter. (TODO)</p>
</body>
</html>
