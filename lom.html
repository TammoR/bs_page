<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>Logical Operator Machines</title>
  <style type="text/css">
      code{white-space: pre-wrap;}
      span.smallcaps{font-variant: small-caps;}
      div.line-block{white-space: pre-line;}
      div.column{display: inline-block; vertical-align: top; width: 50%;}
  </style>
  <link rel="stylesheet" href="./md_html.css">
  <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.2/MathJax.js?config=TeX-AMS_CHTML-full" type="text/javascript"></script>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<header>
<h1 class="title">Logical Operator Machines</h1>
</header>
<nav id="TOC">
<ul>
<li><a href="#generalisation-to-mathematical-operators">Generalisation to Mathematical Operators</a></li>
<li><a href="#equivalence-between-logical-operators-under-inverted-input-or-output.">Equivalence between logical operators under inverted input or output.</a><ul>
<li><a href="#likelihood-functions">Likelihood functions</a></li>
<li><a href="#conditionals">Conditionals</a></li>
<li><a href="#implementation">Implementation</a></li>
</ul></li>
<li><a href="#the-xor-clan">The <code>XOR</code> clan</a></li>
<li><a href="#human-interpretable-lom-semantics">Human interpretable LOM semantics</a></li>
<li><a href="#food-preparation-as-lom-application">Food preparation as LOM application</a></li>
<li><a href="#references">References</a></li>
</ul>
</nav>
<p><em>Logical Operator Machines</em> (LOMs) are a class of probabilistic latent feature models whose generative process is based on the composition of logic gates. These models can be viewed as matrix factorisation models where the product <em>and</em> factor matrices are binary. For a <span class="math inline">\(K\)</span>-dimensional dataset <span class="math inline">\(X \in \{0,1\}^{k_1 \times k_2 \times \ldots \times k_K}\)</span>, a latent dimensionality of <span class="math inline">\(L\)</span>, and an inner and outer logical operator, <code>LOP</code><span class="math inline">\(_i\)</span> and <code>LOP</code><span class="math inline">\(_o\)</span> we have the general form</p>
<p><span class="math display">\[y_{[k]} = \underset{l}{\text{LOP}_o}\left[\underset{k}{\text{LOP}_i}(f_{kl})\right]\;. \]</span></p>
<p>We use a Bernoulli noise model with parameter <span class="math inline">\(\lambda\)</span>, such that</p>
<p><span class="math display">\[p(x_{[k]}|.) = \sigma\left[\lambda \tilde{y}_{[k]} \right]\;. \]</span></p>
<p>Here, <span class="math inline">\([k]\)</span> denotes a tuple of indices <span class="math inline">\([k_1, k_2, \ldots k_K]\)</span>, such that <span class="math inline">\(x_{[k]}\)</span> is a single observation. The OrMachine is particular instance of LOM with: <span class="math inline">\(K=2\)</span>, <code>LOP</code><span class="math inline">\(_i\)</span> = <code>AND</code>, <code>LOP</code><span class="math inline">\(_o\)</span> = <code>OR</code>. <span class="math display">\[y_{nd} = \underset{l}{\text{OR}}\left[\underset{n,d}{\text{AND}}(z_{nl},u_{ld})\right] \]</span></p>
<p>This general view suggests some extensions. First we can extend the OrM to data of arbitrary arity, for <span class="math inline">\(K=3\)</span> we recover TensOrM. More interestingly, we consider various combinations of operator links.</p>
<h2 id="generalisation-to-mathematical-operators">Generalisation to Mathematical Operators</h2>
<p>Over-stretching the notion of logical operators, we have already developed the <code>MAX</code>-<code>AND</code> Machine, where the maximum is taken over dispersion parameters associated to each latent dimension. This leads to a wider class of models, which we refer to as <em>Mathematical Operator Machines</em>. They rely noise model where different dispersion parameters are assigned across one or more dimensions of the factor or product matrix and thus take the general form</p>
<p><span class="math display">\[p(x_{[k]}|.) = \sigma\left[\tilde{x}_{[k]} \underset{l}{\text{MOP}_o}\left(\lambda_l \underset{k}{\text{MOP}_i}(\lambda_k f_{kl})\right)\right] \;. \]</span></p>
<p>This definition can be seen as a further generalisation of LOMs. Although find a general notation with the different <span class="math inline">\(\lambda\)</span>s seems difficult.</p>
<p>The <code>MAX</code>-<code>AND</code> Machine is a hybrid with <code>MOP</code><span class="math inline">\(_o\)</span> = <code>MAX</code>, <code>MOP</code><span class="math inline">\(_i\)</span> = <code>AND</code>, <span class="math inline">\(\lambda_{[k]} = 1 \; \forall \; [k]\)</span>: <span class="math display">\[p(x_{nd}|.) = \sigma\left[\tilde{x}_{[k]} \underset{l}{\text{MAX}}\left(\lambda_l \underset{k=n,d}{\text{AND}}( f_{kl})\right)\right] \;. \]</span></p>
<h2 id="equivalence-between-logical-operators-under-inverted-input-or-output.">Equivalence between logical operators under inverted input or output.</h2>
<p>We limit the discussion to logical operators for now.</p>
<table>
<thead>
<tr class="header">
<th></th>
<th><code>AND</code></th>
<th><code>OR</code></th>
<th><code>XOR</code></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>all zeros</td>
<td>0</td>
<td>0</td>
<td>0</td>
</tr>
<tr class="even">
<td>mixed</td>
<td>0</td>
<td>1</td>
<td>1</td>
</tr>
<tr class="odd">
<td>all ones</td>
<td>1</td>
<td>1</td>
<td>0</td>
</tr>
</tbody>
</table>
<p>Each of these has a negated version <code>NAND</code>, <code>NOR</code> and <code>NXOR</code>. Using these, for our purposes, is equivalent to inverting the output. Moreover, we can invert the inputs, which leads to <span class="math display">\[
\begin{aligned}
\text{AND} &amp;\longleftrightarrow \text{NOR} \\ 
\text{OR}  &amp;\longleftrightarrow \text{NAND} \\ 
\text{XOR} &amp;\longleftrightarrow \text{XOR}
\end{aligned}
\]</span></p>
<p>This also allows us to read off useful relationships. E.g. if we have an <code>OR</code>-<code>AND</code>-Machine, and we want an <code>AND</code>-<code>NOR</code>-Machine, we can invert the input to inner logical operator (i.e. the factor matrices) to get a <code>OR</code>-<code>NOR</code>-Machine. Next, we invert the output of outer logical operator (i.e. the reconstruction of the product matrix) to get a <code>NOR</code>-<code>NOR</code>-Machine. Practically, inverting the input of the inner- and the output of the outer operator amounts to simple pre- and post-processing steps. However, inverting the output of the inner operator or, equivalently, the input of the outer operator needs some extra implementation work. We can formalise these transitions, defining an inversion operator <span class="math inline">\(\mathcal{I}(x) = 1-x\)</span>, that can invert at the input, the intermediate and the output stage.</p>
<p><span class="math display">\[y_{[k]} = \mathcal{I}_o\, \underset{l}{\text{LOP}_o}\, \mathcal{I}_h\, \underset{k}{\text{LOP}_i}(\mathcal{I}_i\,f_{kl})\;. \]</span></p>
<p>Note, that <span class="math inline">\(\mathcal{I}\,\mathcal{I} = 1\)</span>. Thus, given an <code>OR</code>-<code>AND</code>-Machine, we get a <code>NOR</code>-<code>NOR</code>-Machine by settings <span class="math inline">\(\mathcal{I}_o = \mathcal{I}_i = \mathcal{I}\)</span> and <span class="math inline">\(\mathcal{I}_h = 1\)</span>. This allows us to transition between all possible LOMs, except those that include <code>XOR</code> and <code>XNOR</code> gates, which can’t be achieved by mere inversion. The transitions are given in the following table.</p>
<table style="width:53%;">
<colgroup>
<col style="width: 12%" />
<col style="width: 11%" />
<col style="width: 8%" />
<col style="width: 11%" />
<col style="width: 9%" />
</colgroup>
<thead>
<tr class="header">
<th><span class="math inline">\(\mbox{inner}\rightarrow\)</span> <span class="math inline">\(\mbox{outer}\downarrow\)</span></th>
<th><code>AND</code></th>
<th><code>OR</code></th>
<th><code>NAND</code></th>
<th><code>NOR</code></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><code>OR</code></td>
<td>1</td>
<td><span class="math inline">\(\mathcal{I}_h \mathcal{I}_i\)</span></td>
<td><span class="math inline">\(\mathcal{I}_h\)</span></td>
<td><span class="math inline">\(\mathcal{I}_i\)</span></td>
</tr>
<tr class="even">
<td><code>AND</code></td>
<td><span class="math inline">\(\mathcal{I}_o \mathcal{I}_h\)</span></td>
<td><span class="math inline">\(\mathcal{I}_o \mathcal{I}_i\)</span></td>
<td><span class="math inline">\(\mathcal{I}_o\)</span></td>
<td><span class="math inline">\(\mathcal{I}_o \mathcal{I}_h \mathcal{I}_i\)</span></td>
</tr>
<tr class="odd">
<td><code>NAND</code></td>
<td><span class="math inline">\(\mathcal{I}_h\)</span></td>
<td><span class="math inline">\(\mathcal{I}_i\)</span></td>
<td>1</td>
<td><span class="math inline">\(\mathcal{I}_h\mathcal{I}_i\)</span></td>
</tr>
<tr class="even">
<td><code>NOR</code></td>
<td><span class="math inline">\(\mathcal{I}_o\)</span></td>
<td><span class="math inline">\(\mathcal{I}_o \mathcal{I}_h \mathcal{I}_i\)</span></td>
<td><span class="math inline">\(\mathcal{I}_o \mathcal{I}_h\)</span></td>
<td><span class="math inline">\(\mathcal{I}_o \mathcal{I}_i\)</span></td>
</tr>
</tbody>
</table>
<p>[TODO: rewrite this with <code>AND</code>-<code>AND</code> as pivot]</p>
<p>This also highlights the non-obvious equalities, e.g. an <code>OR</code>-<code>AND</code>-Machine is the same as a <code>NAND</code>-<code>NAND</code>-Machine. In the table above, equivalent machines live on the same diagonal. We have: <span class="math display">\[ 
\begin{align}
\texttt{OR-AND} &amp;= \texttt{NAND-NAND}\\
\texttt{OR-OR} &amp;= \texttt{NAND-NOR}\\
\texttt{OR-NAND} &amp;= \texttt{NAND-AND}\\
\texttt{OR-NOR} &amp;= \texttt{NAND-OR} \\
\texttt{AND-AND} &amp;= \texttt{NOR-NAND} \\ 
\texttt{AND-OR} &amp;= \texttt{NOR-NOR}\\
\texttt{AND-NAND} &amp;= \texttt{NOR-AND} \\
\texttt{AND-NOR} &amp;= \texttt{NOR-OR}
\end{align}
\]</span></p>
<h3 id="likelihood-functions">Likelihood functions</h3>
<p>These relationships further manifest in the corresponding model likelihoods. Remember the general form from above: <span class="math display">\[ p(x=1|.) = \sigma(\lambda \tilde{y})\;. \]</span></p>
<p>The simplest expression for <span class="math inline">\(y\)</span> is that of an <code>AND</code>-<code>AND</code>-Machine, where we have</p>
<p><span class="math display">\[
\begin{align}
y_{[k]}^{\texttt{AND-AND}} = \underset{l}{\text{AND}}\left[\underset{k}{\text{AND}}(f_{kl})\right]
        = \prod_l \prod_k f_{kl}
\end{align}
\]</span> We can now apply the inversion operator, just as we did in a more abstract fashion above. Conversion to an <code>OR</code>-<code>AND</code>-Machine amounts to application of <span class="math inline">\(\mathcal{I}_o\)</span> and <span class="math inline">\(\mathcal{I}_h\)</span>, such that <span class="math display">\[
\begin{align}
 y_{[k]}^{\texttt{OR-AND}} = \mathcal{I} \prod_l \mathcal{I} \prod_k f_{kl} =  1 - \prod_l \left(1- \prod_k f_{kl} \right)\;.
\end{align}
\]</span></p>
<p>This is the well-known OrMachine likelihood. To give a further example, the <code>AND</code>-<code>NOR</code>-Machine (= <code>OR</code>-<code>NOR</code>) requires inversion at all three stages: <span class="math display">\[
\begin{align}
 y_{[k]}^{\texttt{AND-OR}} = \mathcal{I} \prod_l \mathcal{I} \prod_k \mathcal{I} f_{kl} =  1 - \prod_l \left[1- \prod_k \left(1-f_{kl}\right) \right]\;.
\end{align}
\]</span></p>
<h3 id="conditionals">Conditionals</h3>
<p>This can be extended to the simple expressions of the full conditional distributions. The general form is given by <span class="math display">\[
p(z_{nl}|.) = \sigma\left[\lambda \tilde{z}_{nl} g_{nl}\right]\;,
\]</span> where we have for the <code>AND</code>-<code>AND</code>-Machine <span class="math display">\[
g_{nl} = \sum\limits_{k \in [k]\setminus n}^{K_k} \tilde{x}_{nd} 
    \left[ \prod\limits_{[k]\setminus n} f_{kl} \right]
    \left[ \prod\limits_{l&#39;\neq l} \prod\limits_{[k]} f_{kl&#39;} \right]\;.
\]</span> The sum is taken over all child nodes of <span class="math inline">\(z_{nl}\)</span>. The first square bracket indicates the effect of <span class="math inline">\(z_{nl}\)</span> on the respective child node as mediated through the matrix entries that connect <span class="math inline">\(z_{nl}\)</span> and <span class="math inline">\(x_{n,[k]\setminus n}\)</span>. The second square bracket indicates the explaining away dependency, i.e. how the state of <span class="math inline">\(z_{nl}\)</span>’s co-parents affect the ability of <span class="math inline">\(z_{nl}\)</span> to change the likelihood. The notation becomes clearer for K=2, where the above equation takes the form</p>
<p><span class="math display">\[
g_{nl} = \sum\limits_{d} \tilde{x}_{nd} \left[ u_{ld} \right]
    \left[ \prod\limits_{l&#39;\neq l} u_{l&#39;d} z_{nl&#39;} \right]\;.
\]</span></p>
<p>We can find the expression for any of the above LOMs by applying inversion operators to the <code>AND</code>-<code>AND</code>-Machine as follows: <span class="math display">\[
g_{nl} = \sum\limits_{k \in [k]\setminus n}^{K_k} [\mathcal{I}_o \mathcal{I}_h x_{nd}]^{\sim} 
    \underbrace{\left[ \prod\limits_{[k]\setminus n} \mathcal{I}_i f_{kl} \right]}_{\text{bracket 1}}
    \underbrace{\left[ \prod\limits_{l&#39;\neq l} \mathcal{I}_h \prod\limits_{[k]} \mathcal{I}_i f_{kl&#39;} \right]}_{\text{bracket 2}}\;.
\]</span> E.g. for the good old <code>OR</code>-<code>AND</code>-Machine we have <span class="math inline">\(\mathcal{I}_o = \mathcal{I}_h = \mathcal{I}\)</span> and <span class="math inline">\(\mathcal{I}_i = 1\)</span>, which leads to <span class="math display">\[
g^{\texttt{OR-AND}}_{nl} =  
    \sum\limits_{k \in [k]\setminus n}^{K_k} x_{nd}^{\sim} 
    \left[ \prod\limits_{[k]\setminus n} f_{kl} \right]
    \left[ \prod\limits_{l&#39;\neq l} 1 - \prod\limits_{[k]} f_{kl&#39;} \right]
    \stackrel{K=2}{\rightarrow}
    \sum\limits_{d} x_{nd}^{\sim} 
    u_{dl}
    \left[ \prod\limits_{l&#39;\neq l} (1 - u_{dl&#39;}z_{nl&#39;}) \right]    
    \;.
\]</span></p>
<h3 id="implementation">Implementation</h3>
<p>Computing the full conditional amounts to checking whether any of the square bracket terms becomes zero. This can be done efficiently, since it’s sufficient if any of the terms inside the products becomes zero. Computationally, this amounts to checking one of the following conditions for each bracket.</p>
<ul>
<li><span class="math inline">\(\exists\; (k \in [k], l): f_{kl}=1\)</span></li>
<li><span class="math inline">\(\exists\; (k \in [k], l): f_{kl}=0\)</span> (OrMachine bracket 1)</li>
<li><span class="math inline">\(\exists\; l: f_{kl}=1 \;\forall\; k \in [k]\)</span> (OrMachine bracket 2)</li>
<li><span class="math inline">\(\exists\; l: f_{kl}=0 \;\forall\; k \in [k]\)</span></li>
</ul>
<h2 id="the-xor-clan">The <code>XOR</code> clan</h2>
<p>Generalisation to LOMs involving <code>XOR</code> or <code>NXOR</code> gates is less straightforward. Products inside the general expression for LOM likelihoods, given above, are replaced by sums that must equal 1 for the output to be 1.</p>
<p><span class="math display">\[ \begin{align} 
y_{[k]}^{\texttt{AND-AND}} &amp;= \prod_l \prod_{[k]} f_{kl} \\
y_{[k]}^{\texttt{XOR-AND}}  &amp;= 
    1 - \min\left[1, \text{abs}\left(1-\sum\limits_l \prod\limits_{[k]} f_{kl}\right)\right] =
    \begin{cases}
    1;\; \text{if}\;  \sum\limits_l \prod\limits_{[k]} f_{kl} = 1 \\
    0;\; \text{else}
    \end{cases} \\
y_{[k]}^{\texttt{AND-XOR}}  &amp;= 
    1 - \min\left[1, \text{abs}\left(1-\prod\limits_l \sum\limits_{[k]} f_{kl}\right)\right] =
    \begin{cases}
    1;\; \text{if}\;  \prod\limits_l \sum\limits_{[k]} f_{kl} = 1 \\
    0;\; \text{else}
    \end{cases}
\end{align} \]</span> In particular the conditionals become difficult to write due to more complicated <em>explaining away</em> dependencies. An observation that is already explained, can be un-explained by activating another latent code, falsifying the <code>XOR</code>: <span class="math display">\[
g_{nl}^{\texttt{XOR-AND}} = \sum\limits \tilde{x}_{nd} u_{ld} h_{lnd} \;,
\]</span> where we define</p>
<span class="math display">\[\begin{align}
h_{lnd} = \begin{cases}
            1;\;\text{if}&amp;\; \prod\limits_{l&#39;\neq l} z_{nl&#39;} u_{l&#39;d} = 0  \\
            0;\;\text{if}&amp;\; \prod\limits_{l&#39;\neq l} z_{nl&#39;} u_{l&#39;d} &gt; 1  \\
            -1;\;\text{if}&amp;\; \prod\limits_{l&#39;\neq l} z_{nl&#39;} u_{l&#39;d} = 1 \\
          \end{cases}
\end{align}\]</span>
<h2 id="human-interpretable-lom-semantics">Human interpretable LOM semantics</h2>
<p>We can understand the conditions for having a positive output in each possible LOM by combining human understandable semantics for each operator. Start with a choice for <code>LOM</code>_<span class="math inline">\(o\)</span>.</p>
<ul>
<li><code>AND</code>: Every</li>
<li><code>OR</code>: At least one …</li>
<li><code>XOR</code>: Exactly one …</li>
<li><code>NAND</code>: Not all …</li>
<li><code>NOR</code>: Not a single…</li>
<li><code>NXOR</code>: None or more than one …</li>
</ul>
<p>Append the phase <em>… latent feature(s) must be active …</em>, pick one of <code>LOM</code>_<span class="math inline">\(i\)</span>:</p>
<ul>
<li><code>AND</code>: … in all factor matrices</li>
<li><code>OR</code>: … in at least one factor matrix</li>
<li><code>XOR</code>: … in exactly one factor matrix</li>
<li><code>NAND</code>: … in less than one <strong>factor</strong> matrix</li>
<li><code>NOR</code>: … no factor matrix at all</li>
<li><code>NXOR</code>: … in none or more than one factor matrices.</li>
</ul>
<h2 id="food-preparation-as-lom-application">Food preparation as LOM application</h2>
<p>We observe multiple individuals shopping in a supermarket and obtain a binary [<code>person</code> <span class="math inline">\(\times\)</span> <code>ingredients</code>] matrix. We can factorise this matrix</p>
<p><span class="math display">\[ [\texttt{person} \times \texttt{ingredients}] \stackrel{\texttt{OR-AND}}{\approx}
    [\texttt{person} \times \texttt{dish}] \cdot [\texttt{dish} \times \texttt{ingredient}]\;,
\]</span> which is naturally done by an <code>OR</code>-<code>AND</code>-Machine. The rational is that any one dish that requires a given ingredient makes the person buy that ingredient. Additional dishes that require the same ingredient will not make a difference.</p>
<p>What if instead we survey peoples recipe books and observe a binary [<code>person</code> <span class="math inline">\(\times\)</span> <code>dish</code>] matrix? Now, the natural link is an <code>AND</code>-<code>AND</code>-Machine, since all ingredients need to be present in order to prepare a dish. <span class="math display">\[ [\texttt{person} \times \texttt{dish}] \stackrel{\texttt{AND-AND}}{\approx}
    [\texttt{person} \times \texttt{ingredients}] \cdot [\texttt{ingredients} \times \texttt{dishes}]\;
\]</span> and similarly (but less intuitive) <span class="math display">\[ [\texttt{ingredient} \times \texttt{dish}] \stackrel{\texttt{AND-AND}}{\approx}
    [\texttt{person} \times \texttt{ingredients}] \cdot [\texttt{person} \times \texttt{dishes}]\;.
\]</span> Thus there is some sort of inverse relationship between <code>AND</code>-<code>AND</code> and <code>OR</code>-<code>AND</code>.</p>
<!-- - `XOR`  explaining each data point by exactly one latent factor. Could be interesting form the pov of quantifying epistemological rather than ontological uncertainty.
 -->
<h1 id="references">References</h1>
</body>
</html>
